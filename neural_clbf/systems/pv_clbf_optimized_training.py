
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PV-focused Neural CLBF training with scaling and a closed-form CLF-QP.
This script is designed to work with the ScaledControlAffineWrapper and the
IEEE39ControlAffineDAE provided by you.

Key features implemented here:
- State scaling (z-coordinates) for better conditioning.
- Autograd-based Jacobian of V (robust; avoids hand-derived JV errors).
- Closed-form half-space CLF-QP with optional box clamp + fallback.
- PV-bus voltage tracking term (keeps |V| near Vref inside safe set).
- Sensible loss weights and a simple but effective training curriculum.

Run (example):
    python pv_clbf_optimized_training.py --epochs 60 --device cuda

Author: Generated by ChatGPT (GPT-5 Pro)
"""
from dataclasses import dataclass
from typing import Tuple, Optional, List
import math
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F

# User system and the scaling wrapper
try:
    from neural_clbf.systems.IEEE39ControlAffineDAE298 import IEEE39ControlAffineDAE
except Exception:
    from IEEE39ControlAffineDAE298 import IEEE39ControlAffineDAE

from scaled_ieee39_ode_wrapper import ScaledControlAffineWrapper, build_state_scales_like


# -------------- Closed-form CLF-QP (one linear constraint) --------------

@torch.no_grad()
def clf_qp_closed_form(LfV: torch.Tensor, LgV: torch.Tensor, V: torch.Tensor,
                       u_ref: torch.Tensor, lam: float, r_penalty: float,
                       u_box: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                       eps: float = 1e-7) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Solve, per batch item, the convex QP with one inequality:
        min_u  0.5||u - u_ref||^2 + 0.5 * p * rho^2
        s.t.   LfV + LgV u + lam V <= rho,   rho >= 0

    Returns:
        u_star: (B, m)
        rho:    (B, 1)
    """
    B, m = LgV.shape
    a = LgV
    b = -(LfV + lam * V)                # RHS for a^T u <= b + rho
    p = max(float(r_penalty), 1e-6)

    # If feasible at u_ref w/ rho=0 => return early
    dot = (a * u_ref).sum(dim=1) - b    # a^T u_ref - b
    denom = (a * a).sum(dim=1) + (1.0 / p)
    mu = torch.clamp(dot / denom, min=0.0)
    u_cf = u_ref - mu.unsqueeze(1) * a
    rho = (mu / p).unsqueeze(1)

    if u_box is not None:
        u_hi, u_lo = u_box
        u_try = torch.min(torch.max(u_cf, u_lo.view(1, -1).to(u_cf)), u_hi.view(1, -1).to(u_cf))
        # Verify constraint; if violated, expand rho minimally
        viol = ((a * u_try).sum(dim=1) - b).clamp_min(0.0)
        rho = viol.unsqueeze(1)
        return u_try, rho
    return u_cf, rho


# -------------- CLF model --------------

class VNet(nn.Module):
    def __init__(self, n: int, width: int = 256, depth: int = 3):
        super().__init__()
        layers = []
        layers += [nn.Linear(n, width), nn.SiLU()]
        for _ in range(depth - 1):
            layers += [nn.Linear(width, width), nn.SiLU()]
        layers += [nn.Linear(width, 1)]
        self.net = nn.Sequential(*layers)

    def forward(self, z: torch.Tensor):
        # Positive CLF via square; this also stabilizes gradients
        phi = self.net(z).squeeze(-1)
        V = 0.5 * phi.pow(2)
        return V, phi


def V_and_grad(Vnet: VNet, z: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    Compute V(z) and grad V w.r.t. z by autograd (stable & correct).
    Returns:
        V:  (B,)
        JV: (B, 1, n)
    """
    z = z.requires_grad_(True)
    V, _ = Vnet(z)
    JV = torch.autograd.grad(V.sum(), z, create_graph=True)[0]  # (B, n)
    return V, JV.unsqueeze(1)


# -------------- Losses --------------

@dataclass
class LossWeights:
    w_goal: float = 5.0
    w_safe: float = 50.0
    w_unsafe: float = 50.0
    w_qp_relax: float = 1.0
    w_descent_lin: float = 10.0
    w_descent_sim: float = 2.0
    w_pv_voltage: float = 25.0
    w_freq: float = 2.0


def boundary_losses(sys: ScaledControlAffineWrapper, V: torch.Tensor, z: torch.Tensor,
                    safe: torch.Tensor, unsafe: torch.Tensor, V_goal: torch.Tensor,
                    c: float = 1.0, eps: float = 1e-2, lw: LossWeights = LossWeights()):
    loss_terms = {}
    # Goal value small
    loss_terms["goal"] = lw.w_goal * V_goal.mean()

    if safe.any():
        safe_violation = F.relu(eps + V[safe] - c)  # keep V <= c in safe region
        loss_terms["safe"] = lw.w_safe * safe_violation.mean()
    else:
        loss_terms["safe"] = torch.zeros((), device=V.device)

    if unsafe.any():
        unsafe_violation = F.relu(eps + c - V[unsafe])  # push V >= c outside
        loss_terms["unsafe"] = lw.w_unsafe * unsafe_violation.mean()
    else:
        loss_terms["unsafe"] = torch.zeros((), device=V.device)

    # PV voltage tracking (inside safe set): |V| ≈ Vref
    # Map z -> x and run the user's algebraic solve (with grads)
    if safe.any():
        x = sys._x_from_z(z[safe])
        n = sys.base.n_gen
        delta_rel, omega, Eqp, Efd, Pm, Pvalve, Ppv, Qpv = sys.base._unpack_state(x[0]).__class__, None, None, None, None, None, None, None
        # batch-wise solve
        Vs = []; ths = []
        for b in range(x.shape[0]):
            d_rel, om, eqp, efd, pm, pval, ppv, qpv = sys.base._unpack_state(x[b])
            d = sys.base._angle_reconstruct(d_rel)
            Vb, thb = sys.base._solve_kcl_newton(d, eqp, ppv, qpv)  # implicit layer gives grads
            Vs.append(Vb.unsqueeze(0)); ths.append(thb.unsqueeze(0))
        V_all = torch.cat(Vs, dim=0)
        # Use Vref (set by base during equilibrium repair); detach magnitude target
        Vref = getattr(sys.base, "Vref", sys.base.Vset).to(V_all)
        pv_v_loss = (V_all - Vref).pow(2).mean()
        loss_terms["pv_voltage"] = lw.w_pv_voltage * pv_v_loss
    else:
        loss_terms["pv_voltage"] = torch.zeros((), device=V.device)

    # Frequency regularization (keep ω ≈ 1 pu)
    omega = sys._x_from_z(z)[:, (sys.base.n_gen - 1):(2 * sys.base.n_gen - 1)]
    loss_terms["freq"] = lw.w_freq * (omega - 1.0).pow(2).mean()
    return loss_terms


def descent_losses(sys: ScaledControlAffineWrapper, V: torch.Tensor, JV: torch.Tensor, z: torch.Tensor,
                   lam: float, r_penalty: float, lw: LossWeights = LossWeights()) -> dict:
    f, g = sys.control_affine_dynamics(z)
    if f.dim() == 3 and f.size(-1) == 1:
        f = f.squeeze(-1)
    LfV = torch.bmm(JV, f.unsqueeze(-1)).squeeze(-1).squeeze(-1)  # (B,)
    LgV = torch.bmm(JV, g).squeeze(1)                             # (B, m)

    # Closed-form QP with u_ref = u_eq (residual policy)
    u_ref = sys.u_eq.expand(z.shape[0], -1)
    u_hi, u_lo = sys.control_limits
    u_star, rho = clf_qp_closed_form(LfV, LgV, V, u_ref, lam, r_penalty, u_box=(u_hi, u_lo))

    # linearized decrease
    Vdot = LfV + (LgV * u_star).sum(dim=1)
    descent_lin = F.relu(0.1 + Vdot + lam * V).mean()

    return {
        "qp_relax": lw.w_qp_relax * rho.mean(),
        "descent_lin": lw.w_descent_lin * descent_lin,
    }


# -------------- Training loop --------------

def sample_batch(sys: ScaledControlAffineWrapper, B: int, around_goal: float = 0.2):
    up, lo = sys.state_limits
    # Mix: 30% around goal (N(0, σ)), 50% safe uniform, 20% near boundary
    m_goal = max(1, int(0.3 * B))
    m_safe = max(1, int(0.5 * B))
    m_bdry = max(1, B - m_goal - m_safe)
    z_goal = torch.randn(m_goal, sys.n_dims, device=up.device, dtype=up.dtype) * around_goal
    z_safe = sys.sample_safe(m_safe)
    z_bdry = sys.sample_boundary(m_bdry)
    z = torch.cat([z_goal, z_safe, z_bdry], dim=0)[:B]
    return z


def train(sys_base: IEEE39ControlAffineDAE,
          device: str = "cpu",
          epochs: int = 60,
          batch_state: int = 1024,
          horizon_steps: int = 0,
          lr: float = 2e-4,
          lam: float = 2.0,
          r_penalty: float = 200.0,
          width: int = 256,
          depth: int = 3,
          seed: int = 0,
          save_path: str = "pv_clbf_optimized.pt"):
    torch.manual_seed(seed)
    sys_base.compute_linearized_controller([sys_base.nominal_params] if hasattr(sys_base, "nominal_params") else [{}])

    # Scaling
    s_vec = build_state_scales_like(sys_base,
                                    delta_rel=0.2, omega=0.02,
                                    Eqp=0.5, Efd=0.5, Pm=0.5, Pvalve=0.5,
                                    Ppv=0.5, Qpv=0.5)
    sys = ScaledControlAffineWrapper(sys_base, state_scales=s_vec).to(device)

    # CLF net
    Vnet = VNet(sys.n_dims, width=width, depth=depth).to(device)

    opt = torch.optim.AdamW(Vnet.parameters(), lr=lr, weight_decay=1e-4, betas=(0.9, 0.99))
    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=max(epochs, 10), eta_min=lr*0.1)
    lw = LossWeights()

    for ep in range(1, epochs + 1):
        Vnet.train()
        z = sample_batch(sys, batch_state).to(device)
        V, JV = V_and_grad(Vnet, z)

        # Masks
        safe = sys.safe_mask(z)
        unsafe = sys.unsafe_mask(z)

        # Goal V at origin
        V_goal, _ = V_and_grad(Vnet, torch.zeros(1, sys.n_dims, device=z.device, dtype=z.dtype))

        Lb = boundary_losses(sys, V, z, safe, unsafe, V_goal, lw=lw)
        Ld = descent_losses(sys, V, JV, z, lam=lam, r_penalty=r_penalty, lw=lw)

        # Optional simulated one-step decrease (short horizon); set horizon_steps>0 to enable
        Lsim = {}
        if horizon_steps > 0:
            dt = float(sys.base.dt)
            zs = z.clone().detach()
            total = torch.zeros((), device=z.device, dtype=z.dtype)
            for _ in range(horizon_steps):
                # compute control at current zs
                V_s, JV_s = V_and_grad(Vnet, zs)
                f, g = sys.control_affine_dynamics(zs)
                if f.dim() == 3 and f.size(-1) == 1: f = f.squeeze(-1)
                LfV = torch.bmm(JV_s, f.unsqueeze(-1)).squeeze(-1).squeeze(-1)
                LgV = torch.bmm(JV_s, g).squeeze(1)
                u_ref = sys.u_eq.expand(zs.shape[0], -1)
                u_hi, u_lo = sys.control_limits
                u_star, _ = clf_qp_closed_form(LfV, LgV, V_s, u_ref, lam, r_penalty, u_box=(u_hi, u_lo))
                # One Euler step (sufficient for the training signal)
                zs = zs + dt * (f + torch.bmm(g, u_star.unsqueeze(-1)).squeeze(-1))
                V_next, _ = V_and_grad(Vnet, zs)
                total = total + F.relu(0.0 + (V_next - V_s) + lam * dt * V_s).mean()
            Lsim["descent_sim"] = LossWeights().w_descent_sim * (total / max(horizon_steps, 1))

        loss = sum(Lb.values()) + sum(Ld.values()) + sum(Lsim.values())
        opt.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(Vnet.parameters(), 1.0)
        opt.step(); sched.step()

        if ep % 5 == 0 or ep == 1:
            with torch.no_grad():
                msg = " | ".join([
                    f"ep {ep:03d}",
                    f"loss {float(loss):.3e}",
                    " ".join([f"{k}:{float(v):.2e}" for k,v in {**Lb, **Ld, **Lsim}.items()])
                ])
                print(msg, flush=True)

    torch.save(Vnet.state_dict(), save_path)
    print(f"[SAVE] Vnet -> {save_path}")
    return Vnet


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--device", type=str, default="cpu")
    ap.add_argument("--epochs", type=int, default=60)
    ap.add_argument("--batch_state", type=int, default=1024)
    ap.add_argument("--horizon_steps", type=int, default=0)
    ap.add_argument("--lr", type=float, default=2e-4)
    ap.add_argument("--lam", type=float, default=2.0)
    ap.add_argument("--r_penalty", type=float, default=200.0)
    ap.add_argument("--width", type=int, default=256)
    ap.add_argument("--depth", type=int, default=3)
    ap.add_argument("--dt", type=float, default=0.001)
    args = ap.parse_args()

    sys = IEEE39ControlAffineDAE(
        nominal_params={
            "pv_ratio": [0.9, 0.9, 0.85, 0.9, 0.85, 0.85, 0.9, 0.9, 0.85, 0.3],
            "T_pv": (0.01, 0.01),
        },
        dt=float(args.dt),
    )
    # Make equilibrium references consistent with current limiter toggles
    if hasattr(sys, "_repair_equilibrium"):
        sys._repair_equilibrium()

    train(sys_base=sys,
          device=args.device,
          epochs=args.epochs,
          batch_state=args.batch_state,
          horizon_steps=args.horizon_steps,
          lr=args.lr,
          lam=args.lam,
          r_penalty=args.r_penalty,
          width=args.width,
          depth=args.depth)


if __name__ == "__main__":
    main()
